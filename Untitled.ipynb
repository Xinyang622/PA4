{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the <start> <end> to the '$'\n",
    "# f2 = open('./input2.txt', 'a')\n",
    "\n",
    "# with open('./input.txt', 'rw') as f:\n",
    "#     line = f.readline()\n",
    "#     while line:\n",
    "#         # print repr(line)\n",
    "#         if '<start>' in line:\n",
    "#             f2.write('$\\r\\n')\n",
    "#         elif '<end>' not in line:\n",
    "#             f2.write(line)\n",
    "#         line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上两块生成新的文件，我把start 和end统一改成了'$'，跑一遍就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2eaca526b1e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input2.txt', 'r') as f:\n",
    "    s_all = f.read()\n",
    "    \n",
    "# s1 = \n",
    "# seq_len = 25\n",
    "# i = 0\n",
    "# data = []\n",
    "# while i + 25 < len(s):\n",
    "#     data.append(list(s[i:i+25]))\n",
    "#     i += 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(s_all)\n",
    "letter2int = {}\n",
    "i = 0\n",
    "for k, v in counter.most_common():\n",
    "    letter2int[k] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505570"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_layers, batch_size):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = 0.3)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, sentence,hidden):\n",
    "        batch_size = sentence.size()[1]\n",
    "        seq_lens = sentence.size()[0]\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        hidden = repackage(hidden)\n",
    "        lstm_out, hidden = self.lstm(\n",
    "            embeds.view(seq_lens, batch_size, -1), hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_lens * batch_size, -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage(h):\n",
    "    if type(h) == autograd.Variable:\n",
    "        return autograd.Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train = s_all[:int(len(s_all) * 0.8)]\n",
    "s_valid = s_all[int(len(s_all) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6319"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(s_all) * 0.8) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return idxs\n",
    "\n",
    "def batch_generator(batch_size, chunk_size, s):\n",
    "    i = 0\n",
    "    data = []\n",
    "    while i + chunk_size * batch_size < len(s) - 1:\n",
    "        lines1 = []\n",
    "        lines2 = []\n",
    "        for j in range(batch_size):\n",
    "            lines1.append(s[i + j * chunk_size: i + (j + 1) * chunk_size])\n",
    "            lines2.append(s[i + j * chunk_size + 1: i + (j + 1) * chunk_size + 1])\n",
    "        X = [prepare_sequence(l, letter2int) for l in lines1]\n",
    "        y = [prepare_sequence(l, letter2int) for l in lines2]\n",
    "        X = np.array(X).transpose(1,0)\n",
    "        y = np.array(y).transpose(1,0)\n",
    "        X = torch.LongTensor(X)\n",
    "        y = torch.LongTensor(y)\n",
    "        i += chunk_size * batch_size\n",
    "        X = autograd.Variable(X.cuda())\n",
    "        y = autograd.Variable(y.cuda())\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for a, b in batch_generator(64, 25, s_train):\n",
    "#     print a.size()\n",
    "#     count += 1\n",
    "# print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "dim = 95\n",
    "HIDDEN_DIM = 100\n",
    "batch_size = 64\n",
    "chunk_size = 25\n",
    "n_layers = 1\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batch = len(s_train) / (64 * 25)\n",
    "n_valid_batch = len(s_valid) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_RNN(EMBEDDING_DIM, HIDDEN_DIM, n_letters, n_letters, n_layers, batch_size)\n",
    "model = model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [0, 0, 0]\n",
    "last_valid_loss = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopped\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "#inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "for epoch in range(n_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    \n",
    "    losses = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_train):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden) # X - 25 * 64,\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda() \n",
    "        y = y.contiguous().view(-1).cuda() # convert label y to contiguous\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss.data[0]\n",
    "    \n",
    "    losses_valid = 0 \n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_valid):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden)\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda()\n",
    "        y = y.contiguous().view(-1).cuda()\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        losses_valid += loss.data[0]\n",
    "    \n",
    "    record = record[1:]\n",
    "    if losses_valid < last_valid_loss:\n",
    "        record.append(1)\n",
    "    else:\n",
    "        record.append(-1)\n",
    "    if sum(record) == -3:\n",
    "        print 'early stopped'\n",
    "        break\n",
    "    last_valid_loss = losses_valid\n",
    "        \n",
    "    print 'training:', losses / n_train_batch\n",
    "    print 'valid:', losses_valid / n_valid_batch\n",
    "\n",
    "# See what the scores are after training\n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404456"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403200"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252 * 64 * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16128"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     5     3  ...      9    40    15\n",
       "   17     3    13  ...      8    11     3\n",
       "   23     2     0  ...      1    21    11\n",
       "       ...          ⋱          ...       \n",
       "    1    13     0  ...      0     3    15\n",
       "   10     0     2  ...      1     2     0\n",
       "    0    11     4  ...      0     0     1\n",
       "[torch.cuda.LongTensor of size 25x64 (GPU 0)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 17\n",
       "  3\n",
       " 13\n",
       " ⋮ \n",
       " 40\n",
       " 15\n",
       " 28\n",
       "[torch.cuda.LongTensor of size 1600 (GPU 0)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 95])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2letter = {val: key for key, val in letter2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2letter[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence2(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='$', predict_len=100, temperature=0.5):\n",
    "    hidden = model.init_hidden(1)\n",
    "    prime_input = prepare_sequence2(prime_str*1, letter2int)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = autograd.Variable(prime_input.cuda())\n",
    "    inp = inp.view(1,-1)\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        # return top_i\n",
    "        # Add predicted character to string and use as next input\n",
    "        temp = int2letter[top_i[0]]\n",
    "        if temp==\"$\":\n",
    "            break\n",
    "        predicted += temp\n",
    "        inp = autograd.Variable(prepare_sequence2(temp, letter2int).cuda())\n",
    "        inp = inp.view(1,-1)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generate(model, prime_str='$', predict_len=1000, temperature=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\r\n",
      "X:5\r\n",
      "C:The of the Mcooll's\r\n",
      "R:polka\r\n",
      "Z:id:hn-polka-43\r\n",
      "M:2/4\r\n",
      "L:1/8\r\n",
      "K:D\r\n",
      "de/f/e/d/e/f/g ged|ce ge|dcB AGF|EF/E/ DA|BA FA|Bd dc|AD FA|BA EG|\r\n",
      "\"F\"AA AB|cA (3Bc|1 A>B A>B|c>de dB|AB AB/c/|dB ABc|AA AB/A/|1 Bd ge|def gag|fd Bg|af gb|ef/e/d BA|AF BA/D|AF AG|FA AB|cB Bd|ef c2 cc|ef/e/ f/g/e/d/e/d/e/f/g/|age eA|A2 AG|AFA AB/A/|A/G/E/F/A/B/D/A/ GF|EG Bd|ed cA|A>B AB/c/d|AF/A/B/G/A B>A|de/d/e/f/ f/e/d/|cBA|Bce dcB|AGA BAA|edc BAd|cA A2|eA BA/G/|AG AG|FGA Bd|ee e>f|edB ec|fe fe/f/|ged ecA|BAB A2G|e2 ce ce|e>fA BA:|2 ed cA|1 d2 de|fe e2|fe dc Bd|ed cB|AB AG/A/|Bd dB|\r\n",
      "P:variations:\r\n",
      "D>D DE/F/ Bc |1 G2 GB/A/ |\r\n",
      "a/g/e/f/ ef/e/ | fe/f/ ge/e/ dB |1 c2 BA|1 GE/2EF GF/F/G/|1 FA AB/A/|Bd d>e|dB cA|\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
