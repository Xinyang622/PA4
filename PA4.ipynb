{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/input.txt', 'r')\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "# for i in range(len(text)):\n",
    "#     if text[i] == '<start>\\r\\n':\n",
    "#         text[i] = '@\\r\\n'\n",
    "#     elif text[i] == '<end>\\r\\n':\n",
    "#         text[i] = '*\\r\\n'\n",
    "#     elif text[i] == '<end>':\n",
    "#         text[i] = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '\\n'.join(text.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary lookup\n",
    "dictionary = {}\n",
    "count = 0\n",
    "for d in data:\n",
    "    if d not in dictionary:\n",
    "        dictionary[d] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = torch.zeros(len(data), len(dictionary))\n",
    "# for i in range(len(data)):\n",
    "#     dataset[i, dictionary[data[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data[0:int(len(data)*0.8)]\n",
    "testset = data[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, hidden_layer):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.hidden_layer)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.hidden_layer, batch, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(self.hidden_layer, batch, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, sentence, hidden):\n",
    "        '''\n",
    "        param: sentence batch*chunk\n",
    "        type: LongTensor Variable\n",
    "        '''\n",
    "        batch = sentence.size(0)\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, hidden = self.lstm(\n",
    "            embeds.view(1, batch, -1), hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(batch, -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "\n",
    "        return tag_scores, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set(chunk, batch):\n",
    "    inp = torch.LongTensor(batch, chunk)\n",
    "    target = torch.LongTensor(batch, chunk)\n",
    "    \n",
    "    for bi in range(batch):\n",
    "        start_index = np.random.randint(0, len(trainset)-chunk)\n",
    "        end_index = start_index + chunk + 1\n",
    "        tmp = trainset[start_index:end_index]\n",
    "        inp[bi] = prepare_sequence(tmp[:-1], dictionary)\n",
    "        target[bi] = prepare_sequence(tmp[1:], dictionary)\n",
    "        \n",
    "    inp = autograd.Variable(inp).cuda()\n",
    "    target = autograd.Variable(target).cuda()\n",
    "    \n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "4.53931762695\n",
      "epoch=1\n",
      "3.62690887451\n",
      "epoch=2\n",
      "5.51109558105\n",
      "epoch=3\n",
      "3.71547180176\n",
      "epoch=4\n",
      "3.37707336426\n",
      "epoch=5\n",
      "3.19658752441\n",
      "epoch=6\n",
      "3.13354187012\n",
      "epoch=7\n",
      "3.063828125\n",
      "epoch=8\n",
      "3.00906066895\n",
      "epoch=9\n",
      "2.97012939453\n",
      "epoch=10\n",
      "2.95710998535\n",
      "epoch=11\n",
      "2.92116516113\n",
      "epoch=12\n",
      "2.90599060059\n",
      "epoch=13\n",
      "2.88698883057\n",
      "epoch=14\n",
      "2.86670623779\n",
      "epoch=15\n",
      "2.85670349121\n",
      "epoch=16\n",
      "2.85456939697\n",
      "epoch=17\n",
      "2.84350585938\n",
      "epoch=18\n",
      "2.83430969238\n",
      "epoch=19\n",
      "2.82057800293\n",
      "epoch=20\n",
      "2.82399536133\n",
      "epoch=21\n",
      "2.80758728027\n",
      "epoch=22\n",
      "2.81134216309\n",
      "epoch=23\n",
      "2.80684783936\n",
      "epoch=24\n",
      "2.80012542725\n",
      "epoch=25\n",
      "2.79262390137\n",
      "epoch=26\n",
      "2.78952331543\n",
      "epoch=27\n",
      "2.78724060059\n",
      "epoch=28\n",
      "2.79014343262\n",
      "epoch=29\n",
      "2.78446990967\n",
      "epoch=30\n",
      "2.78043060303\n",
      "epoch=31\n",
      "2.78165283203\n",
      "epoch=32\n",
      "2.77691345215\n",
      "epoch=33\n",
      "2.77518859863\n",
      "epoch=34\n",
      "2.77684234619\n",
      "epoch=35\n",
      "2.76485321045\n",
      "epoch=36\n",
      "2.76568084717\n",
      "epoch=37\n",
      "2.76646148682\n",
      "epoch=38\n",
      "2.77047332764\n",
      "epoch=39\n",
      "2.76773925781\n",
      "epoch=40\n",
      "2.76971923828\n",
      "epoch=41\n",
      "2.7659085083\n",
      "epoch=42\n",
      "2.76830505371\n",
      "epoch=43\n",
      "2.76033630371\n",
      "epoch=44\n",
      "2.75650909424\n",
      "epoch=45\n",
      "2.761534729\n",
      "epoch=46\n",
      "2.75770050049\n",
      "epoch=47\n",
      "2.75538818359\n",
      "epoch=48\n",
      "2.75549285889\n",
      "epoch=49\n",
      "2.75383453369\n",
      "epoch=50\n",
      "2.74732391357\n",
      "epoch=51\n",
      "2.761980896\n",
      "epoch=52\n",
      "2.75675994873\n",
      "epoch=53\n",
      "2.75119232178\n",
      "epoch=54\n",
      "2.75305389404\n",
      "epoch=55\n",
      "2.75491333008\n",
      "epoch=56\n",
      "2.75617004395\n",
      "epoch=57\n",
      "2.74826507568\n",
      "epoch=58\n",
      "2.75408111572\n",
      "epoch=59\n",
      "2.7489553833\n",
      "epoch=60\n",
      "2.74802062988\n",
      "epoch=61\n",
      "2.75176422119\n",
      "epoch=62\n",
      "2.74722106934\n",
      "epoch=63\n",
      "2.75153533936\n",
      "epoch=64\n",
      "2.75083953857\n",
      "epoch=65\n",
      "2.75594116211\n",
      "epoch=66\n",
      "2.74631713867\n",
      "epoch=67\n",
      "2.75078491211\n",
      "epoch=68\n",
      "2.74874267578\n",
      "epoch=69\n",
      "2.74223815918\n",
      "epoch=70\n",
      "2.7525894165\n",
      "epoch=71\n",
      "2.75203613281\n",
      "epoch=72\n",
      "2.75196624756\n",
      "epoch=73\n",
      "2.75635955811\n",
      "epoch=74\n",
      "2.74685302734\n",
      "epoch=75\n",
      "2.74981933594\n",
      "epoch=76\n",
      "2.75207733154\n",
      "epoch=77\n",
      "2.75108184814\n",
      "epoch=78\n",
      "2.74990478516\n",
      "epoch=79\n",
      "2.74903686523\n",
      "epoch=80\n",
      "2.74117523193\n",
      "epoch=81\n",
      "2.74212249756\n",
      "epoch=82\n",
      "2.74340942383\n",
      "epoch=83\n",
      "2.7497076416\n",
      "epoch=84\n",
      "2.74472198486\n",
      "epoch=85\n",
      "2.74479553223\n",
      "epoch=86\n",
      "2.74746643066\n",
      "epoch=87\n",
      "2.75192108154\n",
      "epoch=88\n",
      "2.74424499512\n",
      "epoch=89\n",
      "2.75578094482\n",
      "epoch=90\n",
      "2.74917694092\n",
      "epoch=91\n",
      "2.74539764404\n",
      "epoch=92\n",
      "2.74427459717\n",
      "epoch=93\n",
      "2.7458480835\n",
      "epoch=94\n",
      "2.7421395874\n",
      "epoch=95\n",
      "2.75023895264\n",
      "epoch=96\n",
      "2.74692718506\n",
      "epoch=97\n",
      "2.74804870605\n",
      "epoch=98\n",
      "2.74231414795\n",
      "epoch=99\n",
      "2.74360992432\n",
      "epoch=100\n",
      "2.74290008545\n",
      "epoch=101\n",
      "2.74075836182\n",
      "epoch=102\n",
      "2.74959106445\n",
      "epoch=103\n",
      "2.74907714844\n",
      "epoch=104\n",
      "2.74771514893\n",
      "epoch=105\n",
      "2.74834747314\n",
      "epoch=106\n",
      "2.74692901611\n",
      "epoch=107\n",
      "2.74104553223\n",
      "epoch=108\n",
      "2.74364898682\n",
      "epoch=109\n",
      "2.7419039917\n",
      "epoch=110\n",
      "2.74330078125\n",
      "epoch=111\n",
      "2.74628540039\n",
      "epoch=112\n",
      "2.74009765625\n",
      "epoch=113\n",
      "2.74143920898\n",
      "epoch=114\n",
      "2.74963378906\n",
      "epoch=115\n",
      "2.73887237549\n",
      "epoch=116\n",
      "2.74706298828\n",
      "epoch=117\n",
      "2.74890625\n",
      "epoch=118\n",
      "2.74349090576\n",
      "epoch=119\n",
      "2.7439730835\n",
      "epoch=120\n",
      "2.7469442749\n",
      "epoch=121\n",
      "2.74396606445\n",
      "epoch=122\n",
      "2.74473052979\n",
      "epoch=123\n",
      "2.74645477295\n",
      "epoch=124\n",
      "2.74586456299\n",
      "epoch=125\n",
      "2.74231903076\n",
      "epoch=126\n",
      "2.7458013916\n",
      "epoch=127\n",
      "2.74246154785\n",
      "epoch=128\n",
      "2.74384918213\n",
      "epoch=129\n",
      "2.73998260498\n",
      "epoch=130\n",
      "2.74997680664\n",
      "epoch=131\n",
      "2.73610534668\n",
      "epoch=132\n",
      "2.7497442627\n",
      "epoch=133\n",
      "2.74576171875\n",
      "epoch=134\n",
      "2.74261016846\n",
      "epoch=135\n",
      "2.73293029785\n",
      "epoch=136\n",
      "2.73942871094\n",
      "epoch=137\n",
      "2.74854919434\n",
      "epoch=138\n",
      "2.74213470459\n",
      "epoch=139\n",
      "2.74538818359\n",
      "epoch=140\n",
      "2.74403900146\n",
      "epoch=141\n",
      "2.74143707275\n",
      "epoch=142\n",
      "2.73930267334\n",
      "epoch=143\n",
      "2.73939300537\n",
      "epoch=144\n",
      "2.73539245605\n",
      "epoch=145\n",
      "2.74168243408\n",
      "epoch=146\n",
      "2.75092498779\n",
      "epoch=147\n",
      "2.74843505859\n",
      "epoch=148\n",
      "2.74542785645\n",
      "epoch=149\n",
      "2.74244171143\n",
      "epoch=150\n",
      "2.74414367676\n",
      "epoch=151\n",
      "2.74338562012\n",
      "epoch=152\n",
      "2.73463104248\n",
      "epoch=153\n",
      "2.74273498535\n",
      "epoch=154\n",
      "2.7421585083\n",
      "epoch=155\n",
      "2.74204467773\n",
      "epoch=156\n",
      "2.73924072266\n",
      "epoch=157\n",
      "2.7323526001\n",
      "epoch=158\n",
      "2.75450408936\n",
      "epoch=159\n",
      "2.74206573486\n",
      "epoch=160\n",
      "2.74097503662\n",
      "epoch=161\n",
      "2.73884216309\n",
      "epoch=162\n",
      "2.74893707275\n",
      "epoch=163\n",
      "2.74590209961\n",
      "epoch=164\n",
      "2.74045074463\n",
      "epoch=165\n",
      "2.74420501709\n",
      "epoch=166\n",
      "2.74141448975\n",
      "epoch=167\n",
      "2.74632232666\n",
      "epoch=168\n",
      "2.74009552002\n",
      "epoch=169\n",
      "2.74109100342\n",
      "epoch=170\n",
      "2.74218597412\n",
      "epoch=171\n",
      "2.74366210938\n",
      "epoch=172\n",
      "2.74609802246\n",
      "epoch=173\n",
      "2.74576843262\n",
      "epoch=174\n",
      "2.73671600342\n",
      "epoch=175\n",
      "2.74062835693\n",
      "epoch=176\n",
      "2.74982116699\n",
      "epoch=177\n",
      "2.75005401611\n",
      "epoch=178\n",
      "2.74552642822\n",
      "epoch=179\n",
      "2.74502807617\n",
      "epoch=180\n",
      "2.74464202881\n",
      "epoch=181\n",
      "2.7408114624\n",
      "epoch=182\n",
      "2.74424530029\n",
      "epoch=183\n",
      "2.74280059814\n",
      "epoch=184\n",
      "2.74655059814\n",
      "epoch=185\n",
      "2.73736053467\n",
      "epoch=186\n",
      "2.74092102051\n",
      "epoch=187\n",
      "2.74500549316\n",
      "epoch=188\n",
      "2.74193359375\n",
      "epoch=189\n",
      "2.74650482178\n",
      "epoch=190\n",
      "2.74613067627\n",
      "epoch=191\n",
      "2.73914276123\n",
      "epoch=192\n",
      "2.74219116211\n",
      "epoch=193\n",
      "2.73916748047\n",
      "epoch=194\n",
      "2.74015625\n",
      "epoch=195\n",
      "2.74080169678\n",
      "epoch=196\n",
      "2.74115325928\n",
      "epoch=197\n",
      "2.73936676025\n",
      "epoch=198\n",
      "2.74406616211\n",
      "epoch=199\n",
      "2.74540435791\n",
      "epoch=200\n",
      "2.74003112793\n",
      "epoch=201\n",
      "2.74088348389\n",
      "epoch=202\n",
      "2.74351623535\n",
      "epoch=203\n",
      "2.74091339111\n",
      "epoch=204\n",
      "2.74511383057\n",
      "epoch=205\n",
      "2.74391540527\n",
      "epoch=206\n",
      "2.73613220215\n",
      "epoch=207\n",
      "2.74949737549\n",
      "epoch=208\n",
      "2.74472229004\n",
      "epoch=209\n",
      "2.74446685791\n",
      "epoch=210\n",
      "2.74755279541\n",
      "epoch=211\n",
      "2.74792419434\n",
      "epoch=212\n",
      "2.73983612061\n",
      "epoch=213\n",
      "2.74523529053\n",
      "epoch=214\n",
      "2.73932434082\n",
      "epoch=215\n",
      "2.74826141357\n",
      "epoch=216\n",
      "2.73895751953\n",
      "epoch=217\n",
      "2.73992279053\n",
      "epoch=218\n",
      "2.73519500732\n",
      "epoch=219\n",
      "2.7414932251\n",
      "epoch=220\n",
      "2.73871063232\n",
      "epoch=221\n",
      "2.74247802734\n",
      "epoch=222\n",
      "2.74065338135\n",
      "epoch=223\n",
      "2.73968566895\n",
      "epoch=224\n",
      "2.74668701172\n",
      "epoch=225\n",
      "2.73766906738\n",
      "epoch=226\n",
      "2.74735229492\n",
      "epoch=227\n",
      "2.74751251221\n",
      "epoch=228\n",
      "2.75498535156\n",
      "epoch=229\n",
      "2.73826080322\n",
      "epoch=230\n",
      "2.74797576904\n",
      "epoch=231\n",
      "2.74457946777\n",
      "epoch=232\n",
      "2.73643066406\n",
      "epoch=233\n",
      "2.74400939941\n",
      "epoch=234\n",
      "2.74507171631\n",
      "epoch=235\n",
      "2.74312042236\n",
      "epoch=236\n",
      "2.7379095459\n",
      "epoch=237\n",
      "2.73837005615\n",
      "epoch=238\n",
      "2.74220367432\n",
      "epoch=239\n",
      "2.74595214844\n",
      "epoch=240\n",
      "2.74169799805\n",
      "epoch=241\n",
      "2.73555908203\n",
      "epoch=242\n",
      "2.74652862549\n",
      "epoch=243\n",
      "2.74153137207\n",
      "epoch=244\n",
      "2.73835662842\n",
      "epoch=245\n",
      "2.73636352539\n",
      "epoch=246\n",
      "2.73876617432\n",
      "epoch=247\n",
      "2.74505157471\n",
      "epoch=248\n",
      "2.74231781006\n",
      "epoch=249\n",
      "2.73286956787\n",
      "epoch=250\n",
      "2.74200592041\n",
      "epoch=251\n",
      "2.7375958252\n",
      "epoch=252\n",
      "2.73909454346\n",
      "epoch=253\n",
      "2.7416897583\n",
      "epoch=254\n",
      "2.74211120605\n",
      "epoch=255\n",
      "2.74334869385\n",
      "epoch=256\n",
      "2.73855072021\n",
      "epoch=257\n",
      "2.74200836182\n",
      "epoch=258\n",
      "2.73941986084\n",
      "epoch=259\n",
      "2.73921722412\n",
      "epoch=260\n",
      "2.74326507568\n",
      "epoch=261\n",
      "2.74190429688\n",
      "epoch=262\n",
      "2.74410797119\n",
      "epoch=263\n",
      "2.7408984375\n",
      "epoch=264\n",
      "2.74049377441\n",
      "epoch=265\n",
      "2.74499725342\n",
      "epoch=266\n",
      "2.74361785889\n",
      "epoch=267\n",
      "2.73803344727\n",
      "epoch=268\n",
      "2.74434204102\n",
      "epoch=269\n",
      "2.74303924561\n",
      "epoch=270\n",
      "2.74132965088\n",
      "epoch=271\n",
      "2.74579528809\n",
      "epoch=272\n",
      "2.74224212646\n",
      "epoch=273\n",
      "2.74938323975\n",
      "epoch=274\n",
      "2.74099822998\n",
      "epoch=275\n",
      "2.74319580078\n",
      "epoch=276\n",
      "2.74283630371\n",
      "epoch=277\n",
      "2.73733795166\n",
      "epoch=278\n",
      "2.74162780762\n",
      "epoch=279\n",
      "2.73913360596\n",
      "epoch=280\n",
      "2.74217132568\n",
      "epoch=281\n",
      "2.74136108398\n",
      "epoch=282\n",
      "2.74125823975\n",
      "epoch=283\n",
      "2.74135925293\n",
      "epoch=284\n",
      "2.73277099609\n",
      "epoch=285\n",
      "2.74648895264\n",
      "epoch=286\n",
      "2.7404776001\n",
      "epoch=287\n",
      "2.73925964355\n",
      "epoch=288\n",
      "2.73733947754\n",
      "epoch=289\n",
      "2.74089263916\n",
      "epoch=290\n",
      "2.74652893066\n",
      "epoch=291\n",
      "2.73974334717\n",
      "epoch=292\n",
      "2.74103363037\n",
      "epoch=293\n",
      "2.74742095947\n",
      "epoch=294\n",
      "2.74074920654\n",
      "epoch=295\n",
      "2.74306671143\n",
      "epoch=296\n",
      "2.73967437744\n",
      "epoch=297\n",
      "2.74259552002\n",
      "epoch=298\n",
      "2.74183898926\n",
      "epoch=299\n",
      "2.74217681885\n"
     ]
    }
   ],
   "source": [
    "batch = 1000\n",
    "chunk = 100\n",
    "\n",
    "model = LSTM(embedding_dim=100, hidden_dim=100, vocab_size=len(dictionary), hidden_layer=1)\n",
    "model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print 'epoch=%d'%(epoch)\n",
    "    \n",
    "    hidden = model.init_hidden(batch)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for c in range(chunk):\n",
    "        sentence_in, targets = random_training_set(chunk, batch)\n",
    "        tag_scores, hidden = model(sentence_in[:,c], hidden)\n",
    "        \n",
    "        loss += loss_function(tag_scores.view(batch, -1), targets[:, c])\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    print loss.data[0]/chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dict = {}\n",
    "\n",
    "for k,v in dictionary.items():\n",
    "    reverse_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='<start>', predict_len=1000, temperature=0.8):\n",
    "    hidden = model.init_hidden(1)\n",
    "    prime_input = autograd.Variable(prepare_sequence(prime_str, dictionary).unsqueeze(0))\n",
    "\n",
    "    prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = reverse_dict[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = autograd.Variable(prepare_sequence(predicted_char, dictionary).unsqueeze(0))\n",
    "        inp = inp.cuda()\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start>\\nAG |Ad| BA3 !bg|16/.!B2dgaroge g/2 | d/ d6\\nA2A |\\n<ere\\nDE2 !f| B F dead | conesigg2| \\nZ:|: | BE/2d |\\nM:\\n<s A8\\nZ:J.fe3 B,| c2e age/ed2 | GAB| |B2 a|\\nFB | |g2| ? G3\\nX:A ond2F2E|ared dBA/2 Ma d2 BAFAGFA |e/d B| B art.fabs\\nc| | f2| cAGED2cgfe2d | eferilou |\\nA | a| d| | |\\nLig A2| A2| fet B>\\nK:105\\nPra2 G2 G c2 BA2 | nd W:j\\nL:ga e A) | \"A | G2B |B |B |c | |2/and A2/e2006/2 e d c c2B/2 Bc2 Ja \\nT:|\\nR:2|\\n<e/e.f/ | | | AB2 f/d BG|  | B d f2G/ d2dee |f2| :2 G2d B2 duvinnmas | 2Be2d g2FG|\\n<s |(3AA=B|c/4GB2 c2 d/f2fd d/ ena Gmancefe -\\nV:holl | B/c2GG>E/dcA/A | (3cABAB2 gg ABA2d agrc2 d | ec  |]\\n| dee)|| | de| H:P:| g|A4|arinst/8\\n<sc arndBd=etefd2 c/a/dc>\\nT: | Miduraefd2|\\n<shndBA dd A edcA Ifrie pas mate2 | (3dd>\\nK:Star\\nK:|Fragfe |e A2 |\\nC\\nV:16 (3ef/gallazuvirgf d2d2elo e\\n<stide2G2 | A| g3 B D2 B2B2 B2|f id4\\ng| c d2 edeena | B2 FG2:D:C|\\nR:Lee B allllenel G d3/F A d Gcec\\'soutatisc GF f d2 |~ge Bc |AF/gfefec2 e alloueaurige2c>\\nT:hn- G/etrar D| d2B/e effe\\nG n-14 cr |G2d2AB2 B G pic6\\n<eG2 G2d (3 |e ertir'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
