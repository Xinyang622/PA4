{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/input.txt', 'r')\n",
    "text = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if text[i] == '<start>\\r\\n':\n",
    "        text[i] = '@\\r\\n'\n",
    "    elif text[i] == '<end>\\r\\n':\n",
    "        text[i] = '*\\r\\n'\n",
    "    elif text[i] == '<end>':\n",
    "        text[i] = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary lookup\n",
    "dictionary = {}\n",
    "count = 0\n",
    "for d in data:\n",
    "    if d not in dictionary:\n",
    "        dictionary[d] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = torch.zeros(len(data), len(dictionary))\n",
    "# for i in range(len(data)):\n",
    "#     dataset[i, dictionary[data[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data[0:int(len(data)*0.8)]\n",
    "testset = data[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, hidden_layer, batch):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.batch = batch\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, hidden_layer)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.hidden_layer, self.batch, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.zeros(self.hidden_layer, self.batch, self.hidden_dim).cuda()))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence)/self.batch, self.batch, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.softmax(tag_space)\n",
    "\n",
    "        return self.hidden, tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "Variable containing:\n",
      " 4.5429\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=1\n",
      "Variable containing:\n",
      " 4.5427\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=2\n",
      "Variable containing:\n",
      " 4.5426\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=3\n",
      "Variable containing:\n",
      " 4.5423\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=4\n",
      "Variable containing:\n",
      " 4.5422\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=5\n",
      "Variable containing:\n",
      " 4.5420\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=6\n",
      "Variable containing:\n",
      " 4.5418\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=7\n",
      "Variable containing:\n",
      " 4.5418\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=8\n",
      "Variable containing:\n",
      " 4.5413\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=9\n",
      "Variable containing:\n",
      " 4.5411\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=10\n",
      "Variable containing:\n",
      " 4.5405\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=11\n",
      "Variable containing:\n",
      " 4.5400\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=12\n",
      "Variable containing:\n",
      " 4.5391\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=13\n",
      "Variable containing:\n",
      " 4.5385\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=14\n",
      "Variable containing:\n",
      " 4.5369\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=15\n",
      "Variable containing:\n",
      " 4.5345\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=16\n",
      "Variable containing:\n",
      " 4.5184\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=17\n",
      "Variable containing:\n",
      " 4.4630\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=18\n",
      "Variable containing:\n",
      " 4.4168\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=19\n",
      "Variable containing:\n",
      " 4.4455\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=20\n",
      "Variable containing:\n",
      " 4.4207\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=21\n",
      "Variable containing:\n",
      " 4.4355\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=22\n",
      "Variable containing:\n",
      " 4.4491\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=23\n",
      "Variable containing:\n",
      " 4.4326\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=24\n",
      "Variable containing:\n",
      " 4.4237\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=25\n",
      "Variable containing:\n",
      " 4.4352\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=26\n",
      "Variable containing:\n",
      " 4.4421\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=27\n",
      "Variable containing:\n",
      " 4.4183\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=28\n",
      "Variable containing:\n",
      " 4.4410\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=29\n",
      "Variable containing:\n",
      " 4.4213\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=30\n",
      "Variable containing:\n",
      " 4.4368\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=31\n",
      "Variable containing:\n",
      " 4.4397\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=32\n",
      "Variable containing:\n",
      " 4.4175\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=33\n",
      "Variable containing:\n",
      " 4.4311\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=34\n",
      "Variable containing:\n",
      " 4.4394\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=35\n",
      "Variable containing:\n",
      " 4.4095\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=36\n",
      "Variable containing:\n",
      " 4.4141\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=37\n",
      "Variable containing:\n",
      " 4.4241\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=38\n",
      "Variable containing:\n",
      " 4.4210\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=39\n",
      "Variable containing:\n",
      " 4.4552\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=40\n",
      "Variable containing:\n",
      " 4.4289\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=41\n",
      "Variable containing:\n",
      " 4.4319\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=42\n",
      "Variable containing:\n",
      " 4.4198\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=43\n",
      "Variable containing:\n",
      " 4.4338\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=44\n",
      "Variable containing:\n",
      " 4.4273\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=45\n",
      "Variable containing:\n",
      " 4.4259\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "epoch=46\n"
     ]
    }
   ],
   "source": [
    "batch = 100\n",
    "chunk = 20\n",
    "\n",
    "model = LSTM(embedding_dim=32, hidden_dim=100, vocab_size=len(dictionary), hidden_layer=1, batch=100)\n",
    "model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print 'epoch=%d'%(epoch)\n",
    "    \n",
    "    num_chunk = len(trainset)/chunk\n",
    "    m = np.random.choice(range(num_chunk), (num_chunk/batch, batch), replace=False)\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    for i in range(m.shape[0]):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        \n",
    "        train = \"\"\n",
    "        targets = \"\"\n",
    "        for j in m[i]:\n",
    "            train += trainset[chunk*j:chunk*j+chunk-1]\n",
    "            targets += trainset[chunk*j+1:chunk*j+chunk]\n",
    "            \n",
    "        sentence_in = autograd.Variable(prepare_sequence(train, dictionary).cuda())\n",
    "        targets = autograd.Variable(prepare_sequence(targets, dictionary).cuda())\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dict = {}\n",
    "\n",
    "for k,v in dictionary.items():\n",
    "    reverse_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = ['@', '\\r', '\\n', 'X', ' ']\n",
    "model.batch = 1\n",
    "model.hidden = model.init_hidden()\n",
    "predict = model(autograd.Variable(prepare_sequence(start, dictionary).cuda()))\n",
    "\n",
    "res = np.random.choice(range(len(dictionary)) ,p = predict[-1].cpu().data.numpy().reshape(-1))\n",
    "\n",
    "reverse_dict[res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
