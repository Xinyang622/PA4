{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the <start> <end> to the '$'\n",
    "f2 = open('./data/input2.txt', 'a')\n",
    "\n",
    "with open('./data/input.txt', 'rw') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        # print repr(line)\n",
    "        if '<start>' in line:\n",
    "            f2.write('$\\r\\n')\n",
    "        elif '<end>' not in line:\n",
    "            f2.write(line)\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上两块生成新的文件，我把start 和end统一改成了'$'，跑一遍就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/input2.txt', 'r') as f:\n",
    "    s_all = f.read()\n",
    "    \n",
    "# s1 = \n",
    "# seq_len = 25\n",
    "# i = 0\n",
    "# data = []\n",
    "# while i + 25 < len(s):\n",
    "#     data.append(list(s[i:i+25]))\n",
    "#     i += 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(s_all)\n",
    "letter2int = {}\n",
    "i = 0\n",
    "for k, v in counter.most_common():\n",
    "    letter2int[k] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_letters = len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letter2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505570"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_layers, batch_size, drop_out):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = drop_out)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda(),\n",
    "                autograd.Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda())\n",
    "\n",
    "    def forward(self, sentence,hidden):\n",
    "        batch_size = sentence.size()[1]\n",
    "        seq_lens = sentence.size()[0]\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        hidden = repackage(hidden)\n",
    "        lstm_out, hidden = self.lstm(\n",
    "            embeds.view(seq_lens, batch_size, -1), hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(seq_lens * batch_size, -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage(h):\n",
    "    if type(h) == autograd.Variable:\n",
    "        return autograd.Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train = s_all[:int(len(s_all) * 0.8)]\n",
    "s_valid = s_all[int(len(s_all) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6319"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(s_all) * 0.8) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return idxs\n",
    "\n",
    "def batch_generator(batch_size, chunk_size, s):\n",
    "    i = 0\n",
    "    data = []\n",
    "    while i + chunk_size * batch_size < len(s) - 1:\n",
    "        lines1 = []\n",
    "        lines2 = []\n",
    "        for j in range(batch_size):\n",
    "            lines1.append(s[i + j * chunk_size: i + (j + 1) * chunk_size])\n",
    "            lines2.append(s[i + j * chunk_size + 1: i + (j + 1) * chunk_size + 1])\n",
    "        X = [prepare_sequence(l, letter2int) for l in lines1]\n",
    "        y = [prepare_sequence(l, letter2int) for l in lines2]\n",
    "        X = np.array(X).transpose(1,0)\n",
    "        y = np.array(y).transpose(1,0)\n",
    "        X = torch.LongTensor(X)\n",
    "        y = torch.LongTensor(y)\n",
    "        i += chunk_size * batch_size\n",
    "        X = autograd.Variable(X.cuda())\n",
    "        y = autograd.Variable(y.cuda())\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for a, b in batch_generator(64, 25, s_train):\n",
    "#     print a.size()\n",
    "#     count += 1\n",
    "# print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "dim = 95\n",
    "HIDDEN_DIM = 100\n",
    "batch_size = 100\n",
    "chunk_size = 25\n",
    "n_layers = 1\n",
    "n_epochs = 300\n",
    "dropout = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batch = len(s_train) / (64 * 25)\n",
    "n_valid_batch = len(s_valid) / (64 * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_RNN(EMBEDDING_DIM, HIDDEN_DIM, n_letters, n_letters, n_layers, batch_size, dropout)\n",
    "model = model.cuda()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [0, 0, 0]\n",
    "last_valid_loss = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc = 0\n",
      "epoc = 1\n",
      "epoc = 2\n",
      "epoc = 3\n",
      "epoc = 4\n",
      "epoc = 5\n",
      "epoc = 6\n",
      "epoc = 7\n",
      "epoc = 8\n",
      "epoc = 9\n",
      "epoc = 10\n",
      "epoc = 11\n",
      "epoc = 12\n",
      "epoc = 13\n",
      "epoc = 14\n",
      "epoc = 15\n",
      "epoc = 16\n",
      "epoc = 17\n",
      "epoc = 18\n",
      "epoc = 19\n",
      "epoc = 20\n",
      "epoc = 21\n",
      "epoc = 22\n",
      "epoc = 23\n",
      "epoc = 24\n",
      "epoc = 25\n",
      "epoc = 26\n",
      "epoc = 27\n",
      "epoc = 28\n",
      "epoc = 29\n",
      "epoc = 30\n",
      "epoc = 31\n",
      "epoc = 32\n",
      "epoc = 33\n",
      "epoc = 34\n",
      "epoc = 35\n",
      "epoc = 36\n",
      "epoc = 37\n",
      "epoc = 38\n",
      "epoc = 39\n",
      "epoc = 40\n",
      "epoc = 41\n",
      "epoc = 42\n",
      "epoc = 43\n",
      "epoc = 44\n",
      "epoc = 45\n",
      "epoc = 46\n",
      "epoc = 47\n",
      "epoc = 48\n",
      "epoc = 49\n",
      "epoc = 50\n",
      "epoc = 51\n",
      "epoc = 52\n",
      "epoc = 53\n",
      "epoc = 54\n",
      "epoc = 55\n",
      "epoc = 56\n",
      "epoc = 57\n",
      "epoc = 58\n",
      "epoc = 59\n",
      "epoc = 60\n",
      "epoc = 61\n",
      "epoc = 62\n",
      "epoc = 63\n",
      "epoc = 64\n",
      "epoc = 65\n",
      "epoc = 66\n",
      "epoc = 67\n",
      "epoc = 68\n",
      "epoc = 69\n",
      "epoc = 70\n",
      "epoc = 71\n",
      "epoc = 72\n",
      "epoc = 73\n",
      "epoc = 74\n",
      "epoc = 75\n",
      "epoc = 76\n",
      "epoc = 77\n",
      "epoc = 78\n",
      "epoc = 79\n",
      "epoc = 80\n",
      "epoc = 81\n",
      "epoc = 82\n",
      "epoc = 83\n",
      "epoc = 84\n",
      "epoc = 85\n",
      "epoc = 86\n",
      "epoc = 87\n",
      "epoc = 88\n",
      "epoc = 89\n",
      "epoc = 90\n",
      "epoc = 91\n",
      "epoc = 92\n",
      "epoc = 93\n",
      "epoc = 94\n",
      "epoc = 95\n",
      "epoc = 96\n",
      "epoc = 97\n",
      "epoc = 98\n",
      "epoc = 99\n",
      "epoc = 100\n",
      "epoc = 101\n",
      "epoc = 102\n",
      "epoc = 103\n",
      "epoc = 104\n",
      "epoc = 105\n",
      "epoc = 106\n",
      "epoc = 107\n",
      "epoc = 108\n",
      "epoc = 109\n",
      "epoc = 110\n",
      "epoc = 111\n",
      "epoc = 112\n",
      "epoc = 113\n",
      "epoc = 114\n",
      "epoc = 115\n",
      "epoc = 116\n",
      "epoc = 117\n",
      "epoc = 118\n",
      "epoc = 119\n",
      "epoc = 120\n",
      "epoc = 121\n",
      "epoc = 122\n",
      "epoc = 123\n",
      "epoc = 124\n",
      "epoc = 125\n",
      "epoc = 126\n",
      "epoc = 127\n",
      "epoc = 128\n",
      "epoc = 129\n",
      "epoc = 130\n",
      "epoc = 131\n",
      "epoc = 132\n",
      "epoc = 133\n",
      "epoc = 134\n",
      "epoc = 135\n",
      "epoc = 136\n",
      "epoc = 137\n",
      "epoc = 138\n",
      "epoc = 139\n",
      "epoc = 140\n",
      "epoc = 141\n",
      "epoc = 142\n",
      "epoc = 143\n",
      "epoc = 144\n",
      "epoc = 145\n",
      "epoc = 146\n",
      "epoc = 147\n",
      "epoc = 148\n",
      "epoc = 149\n",
      "epoc = 150\n",
      "epoc = 151\n",
      "epoc = 152\n",
      "epoc = 153\n",
      "epoc = 154\n",
      "epoc = 155\n",
      "epoc = 156\n",
      "epoc = 157\n",
      "epoc = 158\n",
      "epoc = 159\n",
      "epoc = 160\n",
      "epoc = 161\n",
      "epoc = 162\n",
      "epoc = 163\n",
      "epoc = 164\n",
      "epoc = 165\n",
      "epoc = 166\n",
      "epoc = 167\n",
      "epoc = 168\n",
      "epoc = 169\n",
      "epoc = 170\n",
      "epoc = 171\n",
      "epoc = 172\n",
      "epoc = 173\n",
      "epoc = 174\n",
      "epoc = 175\n",
      "epoc = 176\n",
      "epoc = 177\n",
      "epoc = 178\n",
      "epoc = 179\n",
      "epoc = 180\n",
      "epoc = 181\n",
      "epoc = 182\n",
      "epoc = 183\n",
      "epoc = 184\n",
      "epoc = 185\n",
      "epoc = 186\n",
      "epoc = 187\n",
      "epoc = 188\n",
      "epoc = 189\n",
      "epoc = 190\n",
      "epoc = 191\n",
      "epoc = 192\n",
      "epoc = 193\n",
      "epoc = 194\n",
      "epoc = 195\n",
      "epoc = 196\n",
      "epoc = 197\n",
      "epoc = 198\n",
      "epoc = 199\n",
      "epoc = 200\n",
      "epoc = 201\n",
      "epoc = 202\n",
      "epoc = 203\n",
      "epoc = 204\n",
      "epoc = 205\n",
      "epoc = 206\n",
      "epoc = 207\n",
      "epoc = 208\n",
      "epoc = 209\n",
      "epoc = 210\n",
      "epoc = 211\n",
      "epoc = 212\n",
      "epoc = 213\n",
      "epoc = 214\n",
      "epoc = 215\n",
      "epoc = 216\n",
      "epoc = 217\n",
      "epoc = 218\n",
      "epoc = 219\n",
      "epoc = 220\n",
      "epoc = 221\n",
      "epoc = 222\n",
      "epoc = 223\n",
      "epoc = 224\n",
      "epoc = 225\n",
      "epoc = 226\n",
      "epoc = 227\n",
      "epoc = 228\n",
      "epoc = 229\n",
      "epoc = 230\n",
      "epoc = 231\n",
      "epoc = 232\n",
      "epoc = 233\n",
      "epoc = 234\n",
      "epoc = 235\n",
      "epoc = 236\n",
      "epoc = 237\n",
      "epoc = 238\n",
      "epoc = 239\n",
      "epoc = 240\n",
      "epoc = 241\n",
      "epoc = 242\n",
      "epoc = 243\n",
      "epoc = 244\n",
      "epoc = 245\n",
      "epoc = 246\n",
      "epoc = 247\n",
      "epoc = 248\n",
      "epoc = 249\n",
      "epoc = 250\n",
      "epoc = 251\n",
      "epoc = 252\n",
      "epoc = 253\n",
      "epoc = 254\n",
      "epoc = 255\n",
      "epoc = 256\n",
      "epoc = 257\n",
      "epoc = 258\n",
      "epoc = 259\n",
      "epoc = 260\n",
      "epoc = 261\n",
      "epoc = 262\n",
      "epoc = 263\n",
      "epoc = 264\n",
      "epoc = 265\n",
      "epoc = 266\n",
      "epoc = 267\n",
      "epoc = 268\n",
      "epoc = 269\n",
      "epoc = 270\n",
      "epoc = 271\n",
      "epoc = 272\n",
      "epoc = 273\n",
      "epoc = 274\n",
      "epoc = 275\n",
      "epoc = 276\n",
      "epoc = 277\n",
      "epoc = 278\n",
      "epoc = 279\n",
      "epoc = 280\n",
      "epoc = 281\n",
      "epoc = 282\n",
      "epoc = 283\n",
      "epoc = 284\n",
      "epoc = 285\n",
      "epoc = 286\n",
      "epoc = 287\n",
      "epoc = 288\n",
      "epoc = 289\n",
      "epoc = 290\n",
      "epoc = 291\n",
      "epoc = 292\n",
      "epoc = 293\n",
      "epoc = 294\n",
      "epoc = 295\n",
      "epoc = 296\n",
      "epoc = 297\n",
      "epoc = 298\n",
      "epoc = 299\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "#inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# print(tag_scores)\n",
    "\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print 'epoc = %d' %(epoch)\n",
    "    \n",
    "    losses = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_train):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden) # X - 25 * 64,\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda() \n",
    "        y = y.contiguous().view(-1).cuda() # convert label y to contiguous\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss.data[0]\n",
    "    \n",
    "    losses_valid = 0 \n",
    "    for X, y in batch_generator(batch_size, chunk_size, s_valid):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        # sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        # targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores, hidden = model(X, hidden)\n",
    "        tag_scores = tag_scores.view(-1, dim).cuda()\n",
    "        y = y.contiguous().view(-1).cuda()\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        \n",
    "        loss = loss_function(tag_scores, y)\n",
    "        losses_valid += loss.data[0]\n",
    "    \n",
    "    train_loss.append(losses/n_train_batch)\n",
    "    validation_loss.append(losses_valid/n_valid_batch)\n",
    "    \n",
    "    record = record[1:]\n",
    "    if losses_valid < last_valid_loss:\n",
    "        record.append(1)\n",
    "    else:\n",
    "        record.append(-1)\n",
    "    if sum(record) == -3:\n",
    "        print 'early stopped'\n",
    "        break\n",
    "    last_valid_loss = losses_valid\n",
    "        \n",
    "#     print 'training:', losses / n_train_batch\n",
    "#     print 'valid:', losses_valid / n_valid_batch\n",
    "\n",
    "# See what the scores are after training\n",
    "# inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "# tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "# print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc4c207bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='loss_train')\n",
    "plt.plot(validation_loss, label='loss_validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2letter = {val: key for key, val in letter2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence2(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prime_str='$', predict_len=100, temperature=0.5):\n",
    "    hidden = model.init_hidden(1)\n",
    "    prime_input = prepare_sequence2(prime_str*1, letter2int)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = autograd.Variable(prime_input.cuda())\n",
    "    inp = inp.view(1,-1)\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        # return top_i\n",
    "        # Add predicted character to string and use as next input\n",
    "        temp = int2letter[top_i[0]]\n",
    "        if temp==\"$\":\n",
    "            break\n",
    "        predicted += temp\n",
    "        inp = autograd.Variable(prepare_sequence2(temp, letter2int).cuda())\n",
    "        inp = inp.view(1,-1)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\r\n",
      "X:1\r\n",
      "T:Polka\r\n",
      "Z:id:hn-hornpipe-8\r\n",
      "M:C|\r\n",
      "K:D\r\n",
      "d2 dc|B2G A2A|BAB cde|dBA BAG|ABc d2B|AFA ABc|ed ec|de cB|cB AF|AF G2:|\r\n",
      "|: e2 ed|c2 BA|B/e/d/e/|dB AG|Ad cB|AG FA|FG AB|cA AB|ce dB|B2 A2:|\r\n",
      "|:ag af|ag ag|agf gfe|def gfe|dBc dBA|1 BGA dBA|GBd gdB|cA AF|AF G2:|\r\n",
      "|:ga af|gf ef|ed ed|e2 g>f|ed cB|AF/A/ AG|FE EF|G2 AB|d2 d2 d2|e2 ge|fe dB|AB c/d/ ed|cd/e/ de|fe fg|\r\n",
      "ef ed/B/|AF GB|AF GF/G/|BA G2:|\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = generate(model, prime_str='$', predict_len=1000, temperature=0.5)\n",
    "print pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
